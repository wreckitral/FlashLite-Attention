# FlashLite-Attention

FlashLite-Attention is a simplified reimplementation of modern attention-kernel
optimization techniques (inspired by FlashAttention) designed to run on mid-range
GPUs such as an RTX 3050 4GB. This project is developed as part of my
undergraduate thesis (skripsi), focusing on memory-efficient CUDA kernels for
self-attention in LLM inference.

> [!WARNING]
> This project is on development
